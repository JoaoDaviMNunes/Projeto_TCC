% de precisão = dados os pesos gerados e o arquivo base das notas das empresas, a porcentagem de semelhança nas avaliações

ex.: "MuitoRelevante" == "MuitoRelevante"

Arquivos do Github:
* entradaEmpresas.csv = [6 notas de 0 a 2,notaSoma,notaEmpirica]
* scoresReports.csv = [rep,per,cob,esc,abr,met,notaCPeso,scoreCPeso] p/ cada empresa
* pesosReports.csv = [p1,p2,p3,p4,p5,p6,precisao] (pesosCampos+precisao)
* comparacaoAvaliacoes.csv = [notaOriginal,scoreOriginal,notaCPeso,scoreCPeso,scoreEmpirico]



1o gráfico - Matriz de Confusão - Média da precisão dos N casos (randomGenerationComparison)
2o gráfico - Gráfico de barras de erros - Média da precisão dos N casos (randomGenerationComparison)
3o gráfico - Matriz de Confusão - Melhor caso da precisão dos N casos (randomGenerationComparison)
4o gráfico - Matriz de Confusão - Pior caso da precisão dos N casos (randomGenerationComparison)